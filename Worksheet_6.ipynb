{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3a6efbbd",
      "metadata": {
        "id": "3a6efbbd"
      },
      "outputs": [],
      "source": [
        "\n",
        "# WORKSHEET 6 PART I & II â€“ SIGMOID AND SOFTMAX REGRESSION FROM SCRATCH\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "485a5e3b",
      "metadata": {
        "id": "485a5e3b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Sigmoid (Logistic) function\n",
        "def logistic_function(x):\n",
        "    return 1 / (1 + np.exp(-x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "12b9dd8e",
      "metadata": {
        "id": "12b9dd8e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Log loss for binary classification\n",
        "def log_loss(y_true, y_pred):\n",
        "    y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)\n",
        "    return -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ae44caa5",
      "metadata": {
        "id": "ae44caa5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Average cost function\n",
        "def cost_function(y_true, y_pred):\n",
        "    return np.mean(log_loss(y_true, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cb31f230",
      "metadata": {
        "id": "cb31f230"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Logistic regression cost using parameters\n",
        "def costfunction_logreg(X, y, w, b):\n",
        "    z = np.dot(X, w) + b\n",
        "    y_pred = logistic_function(z)\n",
        "    return cost_function(y, y_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b99ddb47",
      "metadata": {
        "id": "b99ddb47"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Compute gradients for logistic regression\n",
        "def compute_gradient(X, y, w, b):\n",
        "    n = X.shape[0]\n",
        "    y_pred = logistic_function(np.dot(X, w) + b)\n",
        "    grad_w = (1/n) * np.dot(X.T, (y_pred - y))\n",
        "    grad_b = (1/n) * np.sum(y_pred - y)\n",
        "    return grad_w, grad_b\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "091a4ecc",
      "metadata": {
        "id": "091a4ecc"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Gradient descent for logistic regression\n",
        "def gradient_descent(X, y, w, b, alpha, n_iter):\n",
        "    cost_history = []\n",
        "    for _ in range(n_iter):\n",
        "        grad_w, grad_b = compute_gradient(X, y, w, b)\n",
        "        w -= alpha * grad_w\n",
        "        b -= alpha * grad_b\n",
        "        cost_history.append(costfunction_logreg(X, y, w, b))\n",
        "    return w, b, cost_history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f6c4299d",
      "metadata": {
        "id": "f6c4299d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Binary prediction using threshold\n",
        "def prediction(X, w, b, threshold=0.5):\n",
        "    probs = logistic_function(np.dot(X, w) + b)\n",
        "    return (probs >= threshold).astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "348ffbd5",
      "metadata": {
        "id": "348ffbd5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Evaluation metrics for binary classification\n",
        "def evaluate_classification(y_true, y_pred):\n",
        "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
        "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
        "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
        "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
        "    precision = TP / (TP + FP) if (TP + FP) else 0\n",
        "    recall = TP / (TP + FN) if (TP + FN) else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
        "    return {\"confusion_matrix\": [[TN, FP], [FN, TP]], \"precision\": precision, \"recall\": recall, \"f1_score\": f1}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f4a7a71c",
      "metadata": {
        "id": "f4a7a71c"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Softmax function for multiclass classification\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "bb555898",
      "metadata": {
        "id": "bb555898"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Categorical cross-entropy loss\n",
        "def loss_softmax(y_true, y_pred):\n",
        "    return -np.sum(y_true * np.log(y_pred + 1e-10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "91357891",
      "metadata": {
        "id": "91357891"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Cost function for softmax regression\n",
        "def cost_softmax(X, y, W, b):\n",
        "    z = np.dot(X, W) + b\n",
        "    y_pred = softmax(z)\n",
        "    return -np.sum(y * np.log(y_pred + 1e-10)) / X.shape[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "66ab6fdb",
      "metadata": {
        "id": "66ab6fdb"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Compute gradients for softmax regression\n",
        "def compute_gradient_softmax(X, y, W, b):\n",
        "    n = X.shape[0]\n",
        "    z = np.dot(X, W) + b\n",
        "    y_pred = softmax(z)\n",
        "    grad_W = np.dot(X.T, (y_pred - y)) / n\n",
        "    grad_b = np.sum(y_pred - y, axis=0) / n\n",
        "    return grad_W, grad_b\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ad671da1",
      "metadata": {
        "id": "ad671da1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Gradient descent for softmax regression\n",
        "def gradient_descent_softmax(X, y, W, b, alpha, n_iter):\n",
        "    cost_history = []\n",
        "    for _ in range(n_iter):\n",
        "        grad_W, grad_b = compute_gradient_softmax(X, y, W, b)\n",
        "        W -= alpha * grad_W\n",
        "        b -= alpha * grad_b\n",
        "        cost_history.append(cost_softmax(X, y, W, b))\n",
        "    return W, b, cost_history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "b4349c49",
      "metadata": {
        "id": "b4349c49"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Predict class labels using softmax\n",
        "def predict_softmax(X, W, b):\n",
        "    return np.argmax(softmax(np.dot(X, W) + b), axis=1)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}